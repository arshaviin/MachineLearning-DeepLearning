{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "tNDwnQTSqbHt"
      },
      "source": [
        "# 01 - Feature Selection and Dimentionality Reduction\n",
        "** Ecole Centrale Nantes **\n",
        "\n",
        "** Diana Mateus **\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMbWxn-2qbHw"
      },
      "source": [
        "PARTICIPANTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aqsm9aaqbHx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnCKlRJfqbHy"
      },
      "source": [
        "## 1. Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0kufKBqbHz"
      },
      "source": [
        "### 1.1 Linear Regression\n",
        "\n",
        "**a) Run the code to create a toy dataset**. The dataset has 100 points each described by three features $x_1$, $x_2$ and $x_3$. The target value $y$ is continuous and is linearly generated from the three variables (and noise). Notice how the three variables are created.\n",
        "\n",
        "**b)** **Training a linear regression model** Use the scikit learn in-built functions for fiting a linear model to the created dataset \n",
        "\n",
        "``model = LinearRegression()``\n",
        "\n",
        "``model.fit(X,Y)``\n",
        "\n",
        "The goal is to automatically estimate the parameters $w_1$, $w_2$ and $w_3$ of the linear model from the input datamatrix $\\mathbb{X}$ and the target values ${y}$. \n",
        "\n",
        "Look at the documentation of the ``LinearRegression`` function to recover the estimated values of the intercept  (ordonnée à l'origine) $b$ and coefficient $w_1$, $w_2$, $w_3$ parameters.\n",
        "\n",
        "Compare the results with the Ordinary Least Squares analytical solution to recover the model parameters. Hint: ```np.linalg.inv```\n",
        "\n",
        "**c)** **Evaluating the target predictions** \n",
        "Use the ``model.predict`` function to estimate the predictions $y_{hat}$ for the training dataset $X$.\n",
        "\n",
        "- Plot $y$ vs. $y_{hat}$\n",
        "- Compute the mean squared error and the r2 variance error between the estimated and the ground truth outputs. \n",
        "- Describe your findings\n",
        "\n",
        "**d)** **Comparing the ground truth vs the estimated model parameters** \n",
        "- Compare the values of the original parameters to the estimated ones. Are the estimated values for the coefficients $w_1$, $w_2$, $w_3$ and the intercept $b$ close to the original model?\n",
        "- What can you say about the contribution of each input feature to the output?\n",
        "- Explain why do the recovered coefficients may be different to the ones used to generate the data ? Hint: the reason is not the added noise.\n",
        "- How can we improve the interpretability of the weights? at what cost?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ouj9stsAqbH0"
      },
      "outputs": [],
      "source": [
        "#a) Create dataset \n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "size = 100\n",
        "np.random.seed(seed=5)\n",
        " \n",
        "X_seed = np.random.normal(0, 1, size) \n",
        "X1 = X_seed + np.random.normal(0, .1, size)\n",
        "X2 = X_seed + np.random.normal(0, .01, size)\n",
        "X3 = X_seed + np.random.normal(0, .001, size)\n",
        "X = np.array([X1, X2, X3]).T\n",
        "\n",
        "intercept = 10+np.random.normal(0,1, size).reshape((size,1))\n",
        "\n",
        "W = np.ones((3,1))\n",
        "\n",
        "Y = np.matmul(X,W)+ intercept\n",
        "\n",
        "print('Shape of X',np.shape(X))\n",
        "print('Shape of W', np.shape(W))\n",
        "print('Shape of Y', np.shape(Y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0svRBtsqbH1"
      },
      "outputs": [],
      "source": [
        "#b) Fitting a Linear Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td4N2HpwqbH2"
      },
      "outputs": [],
      "source": [
        "#c) Plot y against y*\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\"% mean_squared_error(Yhat, Y))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score: %.2f' % r2_score(Yhat, Y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jSZU_JRqbH3"
      },
      "source": [
        "### 1.2 Ridge and Lasso Regression\n",
        "\n",
        "a) Using the data from above repeat the regression but fitting this time:\n",
        "- a ridge model ```Ridge()```\n",
        "- a Lasso model ```Lasso()```\n",
        "- an Elastic Net model ```ElasticNet()```\n",
        "\n",
        "b) Compare the **prediction errors** (MSE, r2), among the three regularized models.  Use a fixed value of the regularization coefficient ```alpha=0.3``` (alpha corresponds to $\\lambda$ in the lectures). Comment on the results.\n",
        "\n",
        "c) Plot the estimated coefficients against different values of ``alpha``($\\lambda$). Use the following values \n",
        "``` python\n",
        "alphas = np.logspace(-6, 2, 200)\n",
        "```\n",
        "d) **Coefficient estimate error** Mesure the MSE error between the original and estimated parameters for each case. Plot the coefficient error vs alpha.\n",
        "\n",
        "e) **Prediction error** Mesure the MSE error between the ground truth and predicted target for each case. Plot the target error vs alpha.\n",
        "\n",
        "f) Conclude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIgy31ZrqbH3"
      },
      "outputs": [],
      "source": [
        "#a) Ridge Lasso ElasticNet\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fswfFalSqbH4"
      },
      "outputs": [],
      "source": [
        "#b) Errors\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\"% mean_squared_error(Yhat, Y))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score: %.2f\\n' % r2_score(Yhat, Y))\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error ridge: %.2f\"% mean_squared_error(Yhat_ridge, Y))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score ridge: %.2f\\n' % r2_score(Yhat_ridge, Y))\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error lasso: %.2f\"% mean_squared_error(Yhat_lasso, Y))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score lasso: %.2f\\n' % r2_score(Yhat_lasso, Y))\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error enet: %.2f\"% mean_squared_error(Yhat_enet, Y))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score enet: %.2f\\n' % r2_score(Yhat_enet, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk6gNEFEqbH5"
      },
      "outputs": [],
      "source": [
        "#c-d-e)Plot the estimated coefficients and errors against different values of alpha\n",
        "\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "alphas = np.logspace(-6, 2, 200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4fKofTJqbH5"
      },
      "source": [
        "### 1.3 Feature selection  for Heart Disease\n",
        "The dataset ```filtHeartDataSet``` is a filtered version and subset of Heart dataset which contains a binary outcome labels for 299 patients  with chest pain. \n",
        "- A positive target value indicates the presence of heart disease based on an angiographic test, \n",
        "- while a negative target value means no heart disease. \n",
        "\n",
        "The data matrix dataMatrix contains 13 features (measurements) including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.\n",
        "\n",
        "See the following link for the full description\n",
        "http://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
        "\n",
        "**Goal**: Find and retain only the most relevant features to predict heart disease\n",
        "\n",
        "**a)** Run the given code to load and prepare the Heart Disease dataset.\n",
        "\n",
        "**b)** Compute the correlation of the target to every value and between variables **Hint** use ``np.corrcoef`` on the matrix containing both the target values and the variables.\n",
        "\n",
        "**c)** Run the skitlearn example for univariate feature selection with  as criteria.\n",
        "\n",
        "**d)** Modify the example to do feature selection on the Heart Disease dataset. Try the ``f-test`` and ``mutual information`` univariate tests for classification. \n",
        "\n",
        "**e)** Compare the results of d) against those of Lasso and Elastic Net regression.\n",
        "\n",
        "**f)**  What are the most predictive variables to be preserved? Recover the actual name of the variables in each case.How many variables should we keep?\n",
        "\n",
        "**g)** What type of variable selection methods are the univariate test and Lasso and Elastic Net? Which type of method is missing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8DBaD7dqbH6"
      },
      "outputs": [],
      "source": [
        "#a) Load and prepare data\n",
        "#The variables of interest are Xall and yall\n",
        "#The final ordered list of variable names is found in new_columns_2\n",
        "\n",
        "import pandas as pd\n",
        "# Create Pandas dataframe.\n",
        "columns = [\"age\", \"sex\", \"cp\", \"restbp\", \"chol\", \"fbs\", \"restecg\", \n",
        "           \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n",
        "df0     = pd.read_table(\"heart_disease_all14.csv\", sep=',', header=None, names=columns)\n",
        "# Convert categorical variables with more than two values into dummy variables.\n",
        "# Note that variable ca is discrete but not categorical, so we don't convert it.\n",
        "df      = df0.copy()\n",
        "dummies = pd.get_dummies(df[\"cp\"],prefix=\"cp\")\n",
        "df      = df.join(dummies)\n",
        "del df[\"cp\"]\n",
        "del df[\"cp_4.0\"]\n",
        "df      = df.rename(columns = {\"cp_1.0\":\"cp_1\",\"cp_2.0\":\"cp_2\",\"cp_3.0\":\"cp_3\"})\n",
        "\n",
        "dummies = pd.get_dummies(df[\"restecg\"],prefix=\"recg\")\n",
        "df      = df.join(dummies)\n",
        "del df[\"restecg\"]\n",
        "del df[\"recg_0.0\"]\n",
        "df      = df.rename(columns = {\"recg_1.0\":\"recg_1\",\"recg_2.0\":\"recg_2\"})\n",
        "\n",
        "dummies = pd.get_dummies(df[\"slope\"],prefix=\"slope\")\n",
        "df      = df.join(dummies)\n",
        "del df[\"slope\"]\n",
        "del df[\"slope_2.0\"]\n",
        "df      = df.rename(columns = {\"slope_1.0\":\"slope_1\",\"slope_3.0\":\"slope_3\"})\n",
        "\n",
        "dummies = pd.get_dummies(df[\"thal\"],prefix=\"thal\")\n",
        "df      = df.join(dummies)\n",
        "del df[\"thal\"]\n",
        "del df[\"thal_3.0\"]\n",
        "df      = df.rename(columns = {\"thal_6.0\":\"thal_6\",\"thal_7.0\":\"thal_7\"})\n",
        "\n",
        "# Replace response variable values and rename\n",
        "df[\"num\"].replace(to_replace=[1,2,3,4],value=1,inplace=True)\n",
        "df      = df.rename(columns = {\"num\":\"hd\"})\n",
        "\n",
        "# New list of column labels after the above operations\n",
        "new_columns_1 = [\"age\", \"sex\", \"restbp\", \"chol\", \"fbs\", \"thalach\", \n",
        "                 \"exang\", \"oldpeak\", \"ca\", \"hd\", \"cp_1\", \"cp_2\",\n",
        "                 \"cp_3\", \"recg_1\", \"recg_2\", \"slope_1\", \"slope_3\",\n",
        "                 \"thal_6\", \"thal_7\"]\n",
        "\n",
        "print ('\\nNumber of patients in dataframe: %i, with disease: %i, without disease: %i\\n' \\\n",
        "      % (len(df.index),len(df[df.hd==1].index),len(df[df.hd==0].index)))\n",
        "\n",
        "#print (df.head()) # UNCOMMENT FOR MORE INFO ON THE DATASET\n",
        "#print (df.describe())# UNCOMMENT FOR MORE INFO ON THE DATASET\n",
        "\n",
        "# Standardize the dataframe\n",
        "stdcols = [\"age\",\"restbp\",\"chol\",\"thalach\",\"oldpeak\"]\n",
        "nrmcols = [\"ca\"]\n",
        "stddf   = df.copy()\n",
        "stddf[stdcols] = stddf[stdcols].apply(lambda x: (x-x.mean())/x.std())\n",
        "stddf[nrmcols] = stddf[nrmcols].apply(lambda x: (x-x.mean())/(x.max()-x.min()))\n",
        "\n",
        "new_columns_2 = new_columns_1[:9] + new_columns_1[10:]\n",
        "new_columns_2.insert(0,new_columns_1[9])\n",
        "stddf = stddf.reindex(columns=new_columns_2)\n",
        "\n",
        "# Convert dataframe into numpy arrays to be used by classifiers\n",
        "yall = stddf[\"hd\"] # the heart disease column alone\n",
        "Xall = stddf[new_columns_2[1:]].values # the potentially predictive variables\n",
        "yXall = stddf.values # y and X combined in the same matrix, with y in the first column\n",
        "\n",
        "print(\"Number of available features:\", Xall.shape[1])\n",
        "stddf[new_columns_2[1:]].head() #only the variables\n",
        "\n",
        "print(new_columns_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw4SG1_gqbH7"
      },
      "outputs": [],
      "source": [
        "# b) Correlation matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-18veULnqbH8"
      },
      "outputs": [],
      "source": [
        "# b)  Scikit learn example for monovariate feature selection on iris dataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
        "\n",
        "iris = load_iris()\n",
        "Xiris, yiris = iris.data, iris.target\n",
        "print('Shape of the input matrix', X.shape)\n",
        "\n",
        "model = SelectKBest(f_classif, k=2)\n",
        "model.fit(Xiris,yiris)\n",
        "mask = model.get_support()\n",
        "print('Selected best variables',mask)\n",
        "\n",
        "Xiris_new=Xiris[:,mask==True]\n",
        "print('Shape after variable selection',Xiris_new.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9LoTMaMqbH8"
      },
      "outputs": [],
      "source": [
        "# c) Run univariate feature selection on the heart disease dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp1-qoMfqbH9"
      },
      "outputs": [],
      "source": [
        "# d) Lasso and Ridge \n",
        "alphas = np.logspace(-4, 0, 200)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlvbnLA3qbH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW24k4X1qbH-"
      },
      "source": [
        "## 2. Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi87Hsa_qbH-"
      },
      "source": [
        "### 2.1. PCA demo with point cloud\n",
        "Run and analyse the demo code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vg8Y5hAqbH_"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Generating the data\n",
        "np.random.seed(1)\n",
        "X = np.matmul(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
        "plt.plot(X[:, 0], X[:, 1], 'o')\n",
        "plt.axis('equal');\n",
        "\n",
        "#Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "print('Explained variance', pca.explained_variance_)\n",
        "print('PCA components\\n',pca.components_)\n",
        "\n",
        "#Plot principal components\n",
        "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
        "plt.axis('equal');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFjWPTKsqbH_"
      },
      "source": [
        "### 2.2 Own PCA implementation \n",
        "Make your own implementation of the PCA algorithm and compare your results with the above\n",
        "\n",
        "To implement PCA follow the steps bellow\n",
        "- Demean the data (mean=0).\n",
        "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix (Equivalently do Singular Vector Decomposition on the data Matrix). \n",
        "``` np.linalg.eig(cov_mat)```\n",
        "- Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues. Remember k is the number of dimensions of the new feature subspace (k ≤ D) (Check first if already ordered)\n",
        "- Construct the projection matrix T from the selected k eigenvectors.\n",
        "- Transform the original dataset X via T to obtain a k dimensional feature subspace Z\n",
        "\n",
        "Your results should be equivalent to the above, explain any difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHsW_rckqbIA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TDxfQKqqbIA"
      },
      "source": [
        "### 2.3 PCA on digits dataset\n",
        "\n",
        "Apply PCA (yours or the in-built version) on the load_digits dataset\n",
        "\n",
        "**a)** Visualize some of the images for each target value.\n",
        "\n",
        "**b)** Apply PCA to reduce the dimensionality of each vectorized image (1,64) to just 2 dimensions. Plot the projected dataset with a scatter plot in two dimensions, using the labels to color. Comment.\n",
        "\n",
        "Hint for plotting:\n",
        "``` python\n",
        "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', \n",
        "            alpha=0.5,cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar();\n",
        "```\n",
        "\n",
        "**c)** Visualize the cumulative explained variance vs the the number of retained dimensions\n",
        "\n",
        "**d)** For 3 different input images (from different target values) \n",
        "- recostruct and show the full 8x8 image from its 1x2 low-dimensional representation.\n",
        "Hint: Use ```pca.inverse_transform```, \n",
        "\n",
        "- show how the aspect of the reconstructed image changes when increasing the number of retained dimensions.\n",
        "\n",
        "**e)** what does the inverse_transformation function do?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUckP--fqbIA"
      },
      "outputs": [],
      "source": [
        "#a) Load and visualize data\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print('Original size',X.shape)\n",
        "\n",
        "# a) Show some of the images\n",
        "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    r = np.random.randint(1,X.shape[0])\n",
        "    im = X[r,:]\n",
        "    #print(im.shape)\n",
        "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
        "    ax.text(0.95, 0.05, 'n = {0}'.format(r), ha='right',\n",
        "            transform=ax.transAxes, color='green')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIqV1lOKqbIB"
      },
      "outputs": [],
      "source": [
        "#b) 2D projection of the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kdOzuh5qbIB"
      },
      "outputs": [],
      "source": [
        "#c) Cummulative explained variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApbUpXP6qbIB"
      },
      "outputs": [],
      "source": [
        "#d) Inverse transform for reconstruction (from low to high dimensional representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2VSyJAb7qbIB"
      },
      "source": [
        "**BONUS** \n",
        "\n",
        "Try the PCA reduction on a faces dataset\n",
        "* https://sandipanweb.wordpress.com/2018/01/06/eigenfaces-and-a-simple-face-detector-with-pca-svd-in-python/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTwz6zWuqbIC"
      },
      "outputs": [],
      "source": [
        "#Reading the data for the Bonus once downloaded from the website\n",
        "\n",
        "import os\n",
        "import skimage.io as io\n",
        "from skimage.transform import resize, rescale\n",
        "import sys\n",
        "#import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMDIR = \"/Users/dianamateus/DATA/\"\n",
        "SUBDIR = os.path.join(IMDIR, \"faces\")\n",
        "\n",
        "imHeight = 128\n",
        "imWidth = 128\n",
        "\n",
        "numIms=3994 #TODO: read automatically\n",
        "X=np.zeros((numIms,imHeight*imWidth)) \n",
        "\n",
        "fig=plt.figure(figsize=(18, 18))\n",
        "counter=1\n",
        "for root, dirnames, filenames in os.walk(SUBDIR):\n",
        "    # print path to all filenames.\n",
        "    for filename in filenames:\n",
        "        f = os.path.join(root, filename)\n",
        "        #print(f)\n",
        "        fd = open(f, 'rb')\n",
        "        f = np.fromfile(fd, dtype=np.uint8,count=imHeight*imWidth)\n",
        "        im = f.reshape((imHeight, imWidth)) #notice row, column format\n",
        "        fd.close()\n",
        "        X[counter,:] = im.ravel()\n",
        "        if counter < 26:\n",
        "            plt.subplot(5,5,counter)        \n",
        "            #plt.imshow(im, cmap = 'gray')\n",
        "            plt.imshow(X[counter,:].reshape(imHeight,imWidth), cmap = 'gray')\n",
        "            plt.title('Image')\n",
        "        counter = counter+1\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MLXCbI8qbIC"
      },
      "source": [
        "Other faces datsets\n",
        "* https://www.kaggle.com/lalitharajesh/face-recognition-eigenfaces\n",
        "* https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html\n",
        "* http://featureselection.asu.edu/index.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bodvKOYqbID"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f374mc2YqbID"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg588543qbIE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "tf2",
      "language": "python",
      "name": "tf2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}